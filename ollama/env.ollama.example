# ═══════════════════════════════════════════════════════════════
# LOCAL AI MODULE (OPTIONAL)
# ═══════════════════════════════════════════════════════════════
# Enable local AI by running:
#   docker compose -f docker-compose.yml -f docker-compose.ollama.yml up -d
#
# Requires: NVIDIA GPU with 12GB+ VRAM
# See: ollama/LOCAL_AI_SETUP.md

# Ollama Configuration
OLLAMA_HOST=ollama
OLLAMA_PORT=11434
OLLAMA_MODEL=gemma3:12b

# Performance Tuning
# Concurrent requests (reduce for less VRAM)
OLLAMA_NUM_PARALLEL=2
# Enable flash attention (faster inference)
OLLAMA_FLASH_ATTENTION=1
# Max models in memory (1 for single model use)
OLLAMA_MAX_LOADED_MODELS=1

# Alternative models (uncomment to use):
# OLLAMA_MODEL=gemma3:12b-q4_K_M    # Lower VRAM (~7GB)
# OLLAMA_MODEL=llama3.2:3b          # Very fast, lower quality
# OLLAMA_MODEL=mistral:7b           # Good balance
